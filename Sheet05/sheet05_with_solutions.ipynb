{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Computer Vision (Winter Term 2022/23) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5a896fb58e2e26c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 05: Segmentation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b909516194670b69",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before end of **Sunday, December 11, 2022**. If you need help (and Google and other resources were not enough), feel free to use the Stud.IP forum. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f23fe4f5fc608a0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Periodic functions) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy, does not give any points, and is voluntary. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea301a189131ace2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What are periodic functions? Can you provide a definition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c179a0d85e719ea5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A periodic function is a function that repeats it values after a given interval, i.e. considering a real function $f:\\mathbb{R}\\to\\mathbb{R}$, this function will be considered periodic, with period length $\\lambda\\in\\mathbb{R}$, if\n",
    "$$f(x+\\lambda) = f(x)\\qquad\\text{for all $x\\in\\mathbb{R}$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd34893bcdd7d7c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What are *amplitude*, *frequency*, *wave length*, and *phase* of a sine function? How can you change these properties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-353f1c67a9fc45b7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The amplitude of a periodic function is a measure of its change over a single period. For the standard sine curve $x\\mapsto \\sin(x)$, the *peak amplitude*, i.e. the height of the highest peak, is $1$ while the *peak-to-peak amplitude*, i.e. the difference between the hightes peak and the lowest through, is $2$. The *wavelenght* is just the period of the sine function, i.e. $\\lambda = 2\\pi$, while the frequency is its inverse $f=\\frac{1}{2\\pi}$. One can scale the function value to change the amplitude and the argument to change the frequency/wavelength. So \n",
    "$a\\cdot\\sin(\\frac{2\\pi}{w}x)$ will give curve with amplitude $a$ and wavelength $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-20750512d8d75573",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** How are sine and cosine defined for complex arguments? In what sense does this generalize the real case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-877fd6d7323b1978",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A standard way to introduce sine and cosine for complex arguments is based on the fact that the real sine and cosine can be introduced as power series\n",
    "$$\\sin(x) = \\sum _{n=0}^{\\infty }{\\frac {(-1)^{n}}{(2n+1)!}}x^{2n+1}\n",
    "\\qquad\\text{and}\\qquad\n",
    "\\cos(x) = \\sum _{n=0}^{\\infty }{\\frac {(-1)^{n}}{(2n)!}}x^{2n}$$\n",
    "This definition can be directly applied to complex arguments. Many of the known trigonometric properties generalize to this case. For purely real arguments, this definition simplifies to the real case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-794d990c19d6a523",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 1: Scale space (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-83d6764245da430a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**a)** What is a scale space? How does it relate to an image pyramid? How is it computed?\n",
    "\n",
    "For some background on scale spaces, you may have a look at the entry on [Scale space](http://kth.diva-portal.org/smash/get/diva2:441147/FULLTEXT01.pdf) in the Encyclopedia  of  Computer Science and Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96412b05f5330c18",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c96e4a51713157e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**b**) Explain the figure depicted on CV-07 slide 37. How are the zero crossings obtained? Why do they tend to form loops? How can the depicted information be used for segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0109a8835fda66d2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cc50d6524c31176a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**c)** Implement the computation of a scale space. Also include code for highlighting the zero crossings at different scales to produce a visualization similar to the figure on CV-07 slide 37."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d51795cb28af7d09",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sobel() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-50adae75bc4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mscale_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscalespace\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mscale_sobel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msobel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mcrossings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscale_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mcrossings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscale_sobel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mscale_sobel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sobel() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "import imageio as imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage.filters import sobel\n",
    "\n",
    "# load the image\n",
    "img = imageio.imread('images/NewspaperRock.png') / 255.\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# row in the image\n",
    "row = 150\n",
    "\n",
    "# the maximal scale in the scale space\n",
    "max_scale = 40\n",
    "\n",
    "# scale space resolution\n",
    "resolution = 400\n",
    "\n",
    "def compute_scalespace(img, max_scale=2, resolution=200):\n",
    "    scales = np.linspace(0, max_scale, resolution)\n",
    "\n",
    "    scalespace = np.zeros((resolution, ) + img.shape, dtype=img.dtype)\n",
    "    for idx, scale in enumerate(scales):\n",
    "        scalespace[idx] = ndimage.gaussian_filter(img, sigma=[scale,scale], mode='nearest')\n",
    "    return scales, scalespace\n",
    "\n",
    "scales, scalespace = compute_scalespace(img, max_scale=max_scale, resolution=resolution)\n",
    "\n",
    "scale_img = scalespace[:,row,:].copy()\n",
    "scale_sobel = sobel(scale_img, axis=1)\n",
    "crossings = np.zeros_like(scale_img)\n",
    "crossings[:, :-1] = (scale_sobel[:, 1:] * scale_sobel[:, :-1])\n",
    "scale_img[crossings<0] = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.gray()\n",
    "plt.imshow(scale_img, vmin=0, vmax=1, origin='lower')\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "texture",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Texture Segmentation (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "texture-q1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is texture? Try to define it in your own words. Can there be a standard definition? What problems do you expect for texture based segmentation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "texture-a1",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Texture refers to a common property in the distribution of gray values or color in a region. Two regions have the same texture, if they agree in that property. That is not a hard definition, but rather a description of the general idea. It can be fleshed out and made more precise by providing a formal specification of the properties, which can be based on different grounds, e.g. based on structural, stochastic, or spectral aspects. However, the suitability of a definition depends on the context: what is texture in one context can be content in another one. Hence one should not expect a general definition that fits for all applications.\n",
    "\n",
    "Texture is usually not defined for individual pixels but for larger structures. When applied as a homogeneity criterion for region based segmentation, one has to take larger neighborhoods into account. As the segment boundaries are not known in advance (otherwise segmentation would be unnecessary), for boundary pixels such an approach will consider values from multiple segments and hence lead to inconclusive results for such pixels. It is also unsuitable to discover small segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "texture-q2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is a co-occurence matrix? How can it be used to characterize texture?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "texture-a2",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The co-occurence matrix represents the correlation between pixels. Although the computation of co-occurence values can be defined quite generally, one usually only considers correlations over small distances, i.e., considering only neighboring pixels. If the correlation over larger distances is to be detected, one usually applies multi-scale strategies, i.e. resize the image and then apply the neighbor-pixel version. The matrix then holds the number of gray value combinations of such pixels for all pairs of gray values between for a selected region of the image. From that matrix, some more compact features, like energy, contrast, entropy, etc. can be computed that allow to characterize different textures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "texture-q3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "\n",
    "**c)** Implement a function to compute the co-occurence matrix of an image (patch). Apply it and compare your results to (CV-07 slide 54)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "texture-a3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "\n",
    "img = imageio.imread('images/mermaid.png')#, mode='L')\n",
    "\n",
    "def get_patch(img, x, y, size=40):\n",
    "    \"\"\"\n",
    "    Extract a rectangular patch from an image and mark it in the original image.\n",
    "    \n",
    "    Args:\n",
    "        img (nndarray): Input image.\n",
    "        x (uint): X-coordinate.\n",
    "        y (uint): Y-coordinate.\n",
    "        size (uint): Size of the patch.\n",
    "        \n",
    "    Returns:\n",
    "        result: The extracted patch.\n",
    "    \"\"\"\n",
    "    result = img[x:x+size,y:y+size].copy()\n",
    "    img[x:x+size, [y,y+1,y+size,y+size+1]] = 0\n",
    "    img[[x,x+1,x+size,x+size+1], y:y+size] = 0\n",
    "    return result\n",
    "\n",
    "patches = []\n",
    "patches.append(get_patch(img, 50,130))\n",
    "patches.append(get_patch(img, 110,80))\n",
    "patches.append(get_patch(img, 260,340))\n",
    "patches.append(get_patch(img, 310,110))\n",
    "patches.append(get_patch(img, 100,440))\n",
    "\n",
    "\n",
    "def cooccurrence(img, dx=1, dy=1):\n",
    "    \"\"\"\n",
    "    Compute a co-occurence matrix for the given image.\n",
    "    \n",
    "    Args:\n",
    "        img          the grayscale image (uint8)\n",
    "        dx,dy        the offset between the two reference points\n",
    "\n",
    "    Returns:\n",
    "        matrix       the co-occurence matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((256, 256))\n",
    "    # BEGIN SOLUTION\n",
    "    for g1, g2 in np.ndindex(matrix.shape):\n",
    "        matrix[g1, g2] = np.sum(\n",
    "            (img[:img.shape[0] - dy, :img.shape[1] - dx] == g1) & (img[dy:, dx:] == g2)\n",
    "        ) \n",
    "    matrix /= img.size \n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Alternative solution:\n",
    "def cooccurrence2(img, dx=1, dy=1):\n",
    "    \"\"\"\n",
    "    Compute a co-occurence matrix for the given image.\n",
    "    \n",
    "    Args:\n",
    "        img          the grayscale image (uint8)\n",
    "        dx,dy        the offset between the two reference points\n",
    "\n",
    "    Returns:\n",
    "        matrix       the co-occurence matrix\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((256,256))\n",
    "    for r,c in np.ndindex((img.shape[0] - dy, img.shape[1] - dx)):\n",
    "        matrix[img[r, c], img[r + dy, c + dx]] += 1\n",
    "    matrix /= img.size\n",
    "    \n",
    "    # END SOLUTION\n",
    "    return matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.gray()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "i = 0\n",
    "for p in patches:\n",
    "    plt.subplot(len(patches),3,i+1); plt.axis('off'); plt.imshow(p)\n",
    "    # For visualization one may apply some extra me, e.g., logarithmization or binarization\n",
    "    #plt.subplot(len(patches),3,i+2); plt.imshow(np.log(1 + cooccurrence2(p, 0, 1)), interpolation='none')\n",
    "    plt.subplot(len(patches),3,i+2); plt.imshow(cooccurrence2(p, 0, 1), interpolation='none')\n",
    "    plt.subplot(len(patches),3,i+3); plt.imshow(cooccurrence2(p,1,0)>0, interpolation='none')\n",
    "    i += 3\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34fbef915eea84d3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: Edge-based segmentation  (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-60b24f1c2023d2d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### a) Gradients\n",
    "What is the gradient of a pixel? How do we calculate the first, how the second derivative of an image?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c966a0915e987a68",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The gradient of a pixel is given by the difference in contrast to its neighboring pixels (4- or 8-neighborhood). The gradient points into the direction with highest divergence. We can imagine an image as a function consisting of two variables (x- and y-axes) and its color shading in each pixel as the outcome. The whole image presents a landscape of valleys and hills in respect to it shading and coloring. A sobel-filtered image presents the first derivative of each pixel while the laplace-filter creates the second derivative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-509a7e125318987d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Edge linking\n",
    "\n",
    "Describe in your own words the idea of edge linking. What is the goal? Why does it not necessarily yield closed\n",
    "edge contours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-17f4e8a096965ade",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "After initial segmentation, we try to link loose edges.\n",
    "* at the start all pixels are marked as unprocessed\n",
    "* make random pixel a start pixel\n",
    "* from the start pixel, look perpendicular to the gradient of the start pixel if there are other pixels with similar gradient\n",
    "* if yes and unprocessed we add them to the edge and make them the new start pixel\n",
    "* if none can be found we initalize another random start pixel\n",
    "\n",
    "Why does it not necessarily yield closed edge contours?\n",
    "* edge linking only considers unprocessed pixels within a certain proximity\n",
    "* it just searches within the direction of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73c06fc018bbf674",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### c) Zero crossings\n",
    "\n",
    "Explain what zero crossings are. Why does the detection of zero crossings always lead to closed contours?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d7d9e72d52085466",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "A zero crossing is a point where a function switches signs.\n",
    "\n",
    "Why does the detection of zero crossings always lead to closed contours?\n",
    "* intuitive argument: imagine the 2D-function defines a 3D landscape, with function values giving the altitude of the landscape at a given coordinate. Then positive values will be above sea level, while negative values will be below sea level. A zero crossing corresponds to the transition between water and land and this boundary line obviously is continous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e3873885045956b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### d) Zero crossings (implementation)\n",
    "\n",
    "Provide an implementation of the zero crossing procedure described in (CV-07 slide 71). To get sensible results you should smooth the image before applying the Laplacian filter, e.g. using the Laplacian of a Gaussian (you may use buildin functions for the filterings steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-de9b4205b58d45298",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "from skimage.feature import canny\n",
    "from skimage.color import rgb2gray\n",
    "from imageio.v3 import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "img = imread('images/swampflower.png').astype(float)\n",
    "#img = imread('images/peppers.png').astype(float)\n",
    "\n",
    "if len(img.shape)>2:\n",
    "    img = rgb2gray(img)\n",
    "img /= img.max()\n",
    "\n",
    "# Now compute edges and then zero crossings using the 4-neighborhood and the 8-neighborhood\n",
    "# BEGIN SOLUTION\n",
    "\n",
    "# from scipy.ndimage.filters import laplace, gaussian_laplace\n",
    "# smooth the image\n",
    "img_smoothed = filters.gaussian(img, sigma=2) # or 2.0, 4.0\n",
    "\n",
    "\n",
    "# detect edges using a laplacian filter\n",
    "edges = filters.laplace(img_smoothed)\n",
    "\n",
    "#hist, _ = np.histogram(img, 256, (0, 255))\n",
    "\n",
    "# N4 neighborhood\n",
    "zero_crossings_n4 = (edges[:-1, 1:] * edges[1:, 1:] <= 0) | (edges[1:, :-1] * edges[1:, 1:] <= 0)\n",
    "\n",
    "# N8 neighborhood\n",
    "zero_crossings_n8 = (zero_crossings_n4[:, 1:] \n",
    "                     | (edges[:-1, 1:-1] * edges[1:, :-2] <= 0) \n",
    "                     | (edges[:-1, 1:-1] * edges[1:, 2:] <= 0))\n",
    "\n",
    "# compute threhshold from histogram and \n",
    "# erase zero crossings where gradient magnitude is below threshold\n",
    "grad_mag = filters.sobel(img_smoothed)[1:,2:]\n",
    "grad_mag_thresh = filters.threshold_otsu(grad_mag)\n",
    "print(f\"Gradient magnitude threshold: {grad_mag_thresh}\")\n",
    "zero_crossings_n8_thresh = zero_crossings_n8.copy()\n",
    "zero_crossings_n8_thresh[grad_mag<grad_mag_thresh] = 0\n",
    "# END SOLUTION\n",
    "\n",
    "plt.figure(figsize=(12, 24))\n",
    "plt.gray()\n",
    "\n",
    "plt.subplot(4,2,1); plt.axis('off'); plt.imshow(img); plt.title('original')\n",
    "plt.subplot(4,2,2); plt.axis('off'); plt.imshow(edges); plt.title('edges')\n",
    "plt.subplot(4,2,3); plt.axis('off'); plt.imshow(zero_crossings_n4); plt.title('zero crossings (N4)')\n",
    "plt.subplot(4,2,4); plt.axis('off'); plt.imshow(zero_crossings_n8); plt.title('zero crossings (N8)' )\n",
    "plt.subplot(4,2,5); plt.axis('off'); plt.imshow(grad_mag); plt.title('Gradient Magnitude' )\n",
    "plt.subplot(4,2,6); plt.hist(grad_mag.flatten(), 255, (0, grad_mag.max())); plt.title('Histogram Gradient Magnitude' )\n",
    "plt.subplot(4,2,7); plt.axis('off'); plt.imshow(zero_crossings_n8_thresh); plt.title('zero crossings (N8) Thresh' )\n",
    "plt.subplot(4,2,8); plt.axis('off'); plt.imshow(canny(img,sigma=2)); plt.title('Canny')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00b8626e22b568b7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 4: Watershed transform  (5 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7554fc226cb5570a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a) Watershed transform\n",
    "\n",
    "Explain in your own words the idea of watershed transform. How do the two different approaches from the lecture work? Why does watershed transform always give a closed contour?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-eedef8dcca391a12",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "Idea of watershed:\n",
    "Find the boundaries were water would meet if the whole region was iteratively flooded.\n",
    "\n",
    "How do the two different approaches from the lecture work?\n",
    "* rain: assume that it rains on all pixels. Water flows opposite to the gradient.\n",
    "* flood: ground water level is contionously rising\n",
    "\n",
    "Why does watershed transform always give a closed contour?\n",
    "* The image is \"continous\" in both dimensions there will always be a local maximum, separating two basins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8119b7b7b3efcacc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b) Implementation\n",
    "\n",
    "Now implement the watershed transform using the flooding approach (CV-07 slide 76, but note, that the algorithm presented there is somewhat simplified!). Obviously, buildin functions for computing watershed transform are not allowed, but all other functions may be used. In this example we appply the watershed transform to a distance transformed image, so you **do not** have to take the gradient image, but can apply the watershed transform directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9e15d7hjee9ad9ff28",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def watershed(img, step=1):\n",
    "    \"\"\"\n",
    "    Perform watershed transform on a grayscale image.\n",
    "    \n",
    "    Args:\n",
    "        img (ndarray): The grayscale image.\n",
    "        step (int): The rise of the waterlevel at each step. Default 1.\n",
    "        \n",
    "    Returns:\n",
    "        edges (ndarray): A binary image containing the watersheds.\n",
    "    \"\"\"\n",
    "\n",
    "    NO_LABEL = 0\n",
    "    WATERSHED = 1\n",
    "    new_label = 2\n",
    "\n",
    "    # initialize labels\n",
    "    label = np.zeros(img.shape, np.uint16)\n",
    "\n",
    "    # BEGIN SOLUTION\n",
    "    for level in np.arange(np.ceil(img.min()), np.ceil(img.max()) + 1, step):\n",
    "        # Remember highest label that was assigned in the last iteration.\n",
    "        previous_new_label = new_label\n",
    "        \n",
    "        # Iterate over all unlabeled pixels below water level\n",
    "        for r, c in np.argwhere((img <= level) & (label == NO_LABEL)):\n",
    "            # Get labels of neighbors.\n",
    "            neighbors = label[max(0, r - 1):min(img.shape[0], r + 2),\n",
    "                              max(0, c - 1):min(img.shape[1], c + 2)]\n",
    "            flooded = np.unique(neighbors[neighbors > WATERSHED])\n",
    "            \n",
    "            if flooded.size == 0:\n",
    "                # Pixel is isoloated: invent a new label.\n",
    "                new_label += 1\n",
    "                label[r, c] = new_label\n",
    "            elif flooded.size == 1:\n",
    "                # Pixel has homogenous neighborhood.\n",
    "                label[r, c] = flooded[0]\n",
    "            else:\n",
    "                # Pixel has inhomogenous neighborhood.\n",
    "                old = flooded[flooded <  previous_new_label] # Neighbors flooded with previous waterlevels.\n",
    "                new = flooded[flooded >= previous_new_label] # Neighbors flooded with current waterlevel.\n",
    "                # If region is flooded for the first time, give all newly flooded neighbors the same label.\n",
    "                if old.size == 0:\n",
    "                    label[r, c] = new[0]\n",
    "                    label[np.isin(label, new[1:])] = label[r, c]\n",
    "                # If parts of the region were flooded before, propagate old label\n",
    "                # to newly flooded pixels.\n",
    "                elif old.size == 1:\n",
    "                    label[r, c] = old[0]\n",
    "                    label[np.isin(label, new)] = label[r, c]\n",
    "                # Two or more independtly flooded regions meet here,\n",
    "                # hence the pixel is a watershed.\n",
    "                else:\n",
    "                    label[r, c] = WATERSHED            \n",
    "    edges = (label == WATERSHED)\n",
    "    return edges\n",
    "    # END SOLUTION\n",
    "\n",
    "\n",
    "img = imageio.imread('images/dist_circles.png')\n",
    "\n",
    "plt.gray()\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(watershed(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c422113ff9318d7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### c) Application: maze\n",
    "\n",
    "You can use watershed transform to find your way through a maze. To do so, first apply a distance transform to the maze and the flood the result. The watershed will show you the way through the maze. Explain why this works.\n",
    "You can use build-in functions instead of your own watershed function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1c20c149aa872621",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio.v3 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "%matplotlib inline\n",
    "\n",
    "img = imageio.imread('images/maze2.png') # 'maze1.png' or 'maze2.png'\n",
    "\n",
    "dt = distance_transform_edt(img)\n",
    "#result = img[:, :, np.newaxis].repeat(3, 2)\n",
    "result = dt[:,:,np.newaxis].repeat(3,2) / 255\n",
    "# BEGIN SOLUTION\n",
    "#result[:,:,:] = dt\n",
    "result[watershed(dt), 1:3] = 0\n",
    "# END SOLUTION\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.title('Solution')\n",
    "plt.axis('off')\n",
    "plt.gray()\n",
    "plt.imshow(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e1faf216fbeaf2a",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "The distance transformed image has its highest values when the points are most distance to the edges.\n",
    "The watershed transform will mark those local maxima as a way through the maze."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
